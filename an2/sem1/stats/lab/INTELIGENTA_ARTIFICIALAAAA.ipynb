{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a48fac0-293a-4937-85ec-a9748bb03b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/silviu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "english_stop_words = stopwords.words(\"english\")\n",
    "\n",
    "data = pd.read_csv(\"enronSpamSubset.csv\")\n",
    "train_data, test_data = train_test_split(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a12b91af-f767-4ec4-8cf9-8ca859625109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLASIFICATORUL_INTELIGENT():\n",
    "    def __init__(self, data, no_classes=2):\n",
    "        self.data = data\n",
    "        self.no_classes = no_classes\n",
    "        self.prob_map = {i: {} for i in range(no_classes)}\n",
    "        self.spam_words = []\n",
    "        self.ham_words = []\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        for row_index, row in tqdm(self.data.iterrows()):\n",
    "            body = row[\"Body\"]\n",
    "            label = row[\"Label\"]\n",
    "            \n",
    "            words = self.clean_body(body)\n",
    "            \n",
    "            if label == 1: # verif daca este spam\n",
    "                self.spam_words.append(words)\n",
    "            else:\n",
    "                self.ham_words.append(words)\n",
    "\n",
    "        self.print_stats()\n",
    "        self.spam_words = list(itertools.chain.from_iterable(self.spam_words))\n",
    "        self.ham_words = list(itertools.chain.from_iterable(self.ham_words))\n",
    "        \n",
    "    def predict(self, body):\n",
    "        words = self.clean_body(body)\n",
    "        \n",
    "        ham_log_prob = self._handle_predict(words, 0)\n",
    "        spam_log_prob = self._handle_predict(words, 1)\n",
    "\n",
    "        return np.argmax([ham_log_prob, spam_log_prob])\n",
    "        \n",
    "        \n",
    "    def _handle_predict(self, words, class_index):\n",
    "        log_probs = []\n",
    "        for word in words:\n",
    "            if word in self.prob_map[class_index]:\n",
    "                proba = self.prob_map[class_index][word]\n",
    "            else:\n",
    "                proba = 1e-8\n",
    "            \n",
    "            log_probs.append(np.log(proba))\n",
    "        return np.sum(log_probs)\n",
    "        \n",
    "                \n",
    "    def fit(self):\n",
    "        smoothing_factor = len(np.unique(self.spam_words + self.ham_words))\n",
    "        \n",
    "        self._handle_fit(self.spam_words, 1, smoothing_factor)\n",
    "        self._handle_fit(self.ham_words, 0, smoothing_factor)\n",
    "        \n",
    "    def _handle_fit(self, words, class_index, smoothing_factor):\n",
    "        from collections import Counter\n",
    "        words_counter = Counter(words)\n",
    "        for word in words_counter:\n",
    "            self.prob_map[class_index][word] = (words_counter[word] + 1)/(len(words) + smoothing_factor)\n",
    "    \n",
    "    def print_stats(self):\n",
    "        self._stat_helper(self.spam_words, \"<SPAM>\")\n",
    "        self._stat_helper(self.ham_words, \"<HAM>\")\n",
    "        \n",
    "    def _stat_helper(self, matrix, class_name):\n",
    "        lens = list(map(lambda x: len(x), matrix))\n",
    "        print(f\"FACEM STAT PENTRU: {class_name}\")\n",
    "        print(f\"Media de cuvinte este: {np.mean(lens)}\")\n",
    "        print(\"==========================================\")\n",
    "        \n",
    "    def clean_body(self, body):\n",
    "        words = wordpunct_tokenize(body) # avem cuvinte\n",
    "        words = self.make_lower(words)\n",
    "        words = self.remove_stop_words(words)\n",
    "        words = self.remove_punctuation(words)\n",
    "        words = self.replace_num(words)\n",
    "        \n",
    "        return words\n",
    "        \n",
    "    def make_lower(self, words):\n",
    "        return [word.lower() for word in words]\n",
    "    \n",
    "    def remove_stop_words(self, words):\n",
    "        return [word for word in words if word not in english_stop_words]\n",
    "    \n",
    "    def remove_punctuation(self, words):\n",
    "        return [word for word in words if word not in string.punctuation]\n",
    "    \n",
    "    def replace_num(self, words):\n",
    "        return [\"<SUPERBET>\" if word.isdigit() else word for word in words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ccb60c0-c697-46f8-8fbc-637bbb95cead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:06, 1508.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACEM STAT PENTRU: <SPAM>\n",
      "Media de cuvinte este: 139.0178\n",
      "==========================================\n",
      "FACEM STAT PENTRU: <HAM>\n",
      "Media de cuvinte este: 181.0638\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "clasificatorul = CLASIFICATORUL_INTELIGENT(data, 2)\n",
    "clasificatorul.prepare_data()\n",
    "clasificatorul.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33617fde-bfc7-49c8-90f7-591ab4f6dbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clasificatorul.predict(\"capitalism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d344096e-be29-4715-8d60-ea44652de8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1241\n",
      "           1       1.00      0.99      0.99      1259\n",
      "\n",
      "    accuracy                           0.99      2500\n",
      "   macro avg       0.99      0.99      0.99      2500\n",
      "weighted avg       0.99      0.99      0.99      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evalueaza_model(model, test_data):\n",
    "    from sklearn.metrics import classification_report\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for row_index, row in test_data.iterrows():\n",
    "        \n",
    "        body = row[\"Body\"]\n",
    "        label = row[\"Label\"]\n",
    "        \n",
    "        true_labels.append(label)\n",
    "        predicted_labels.append(model.predict(body))\n",
    "    \n",
    "    print(classification_report(true_labels, predicted_labels))\n",
    "    \n",
    "evalueaza_model(clasificatorul, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e80d71-8816-4a7e-bee8-5ae51dc69235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
